{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: min_count, stop_words, classifier articture\n",
    "\n",
    "# Import packages\n",
    "import re\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled train data shape: (25000, 3)\n",
      "         id  sentiment                                             review\n",
      "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
      "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
      "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
      "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
      "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
      "\n",
      "Unlabeled train data shape: (50000, 2)\n",
      "          id                                             review\n",
      "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
      "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
      "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
      "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
      "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ...\n",
      "\n",
      "Test data shape: (25000, 2)\n",
      "           id                                             review\n",
      "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
      "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
      "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
      "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
      "4   \"12128_7\"  \"A very accurate depiction of small time mob l...\n"
     ]
    }
   ],
   "source": [
    "# Explore Data\n",
    "labeled_train = pd.read_csv('labeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv('unlabeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('testData.tsv', delimiter='\\t', quoting=3)\n",
    "\n",
    "print 'Labeled train data shape: ' + str(labeled_train.shape)\n",
    "print labeled_train.head()\n",
    "print '\\nUnlabeled train data shape: ' + str(unlabeled_train.shape)\n",
    "print unlabeled_train.head()\n",
    "print '\\nTest data shape: ' + str(test.shape)\n",
    "print test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled train reviews shape: 25000\n",
      "TaggedDocument([u'stuff', u'going', u'moment', u'mj', u'started', u'listening', u'music', u'watching', u'odd', u'documentary', u'watched', u'wiz', u'watched', u'moonwalker', u'maybe', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'really', u'cool', u'eighties', u'maybe', u'make', u'mind', u'whether', u'guilty', u'innocent', u'moonwalker', u'part', u'biography', u'part', u'feature', u'film', u'remember', u'going', u'see', u'cinema', u'originally', u'released', u'subtle', u'messages', u'mj', u'feeling', u'towards', u'press', u'also', u'obvious', u'message', u'drugs', u'bad', u'kay', u'visually', u'impressive', u'course', u'michael', u'jackson', u'unless', u'remotely', u'like', u'mj', u'anyway', u'going', u'hate', u'find', u'boring', u'may', u'call', u'mj', u'egotist', u'consenting', u'making', u'movie', u'mj', u'fans', u'would', u'say', u'made', u'fans', u'true', u'really', u'nice', u'actual', u'feature', u'film', u'bit', u'finally', u'starts', u'minutes', u'excluding', u'smooth', u'criminal', u'sequence', u'joe', u'pesci', u'convincing', u'psychopathic', u'powerful', u'drug', u'lord', u'wants', u'mj', u'dead', u'bad', u'beyond', u'mj', u'overheard', u'plans', u'nah', u'joe', u'pesci', u'character', u'ranted', u'wanted', u'people', u'know', u'supplying', u'drugs', u'etc', u'dunno', u'maybe', u'hates', u'mj', u'music', u'lots', u'cool', u'things', u'like', u'mj', u'turning', u'car', u'robot', u'whole', u'speed', u'demon', u'sequence', u'also', u'director', u'must', u'patience', u'saint', u'came', u'filming', u'kiddy', u'bad', u'sequence', u'usually', u'directors', u'hate', u'working', u'one', u'kid', u'let', u'alone', u'whole', u'bunch', u'performing', u'complex', u'dance', u'scene', u'bottom', u'line', u'movie', u'people', u'like', u'mj', u'one', u'level', u'another', u'think', u'people', u'stay', u'away', u'try', u'give', u'wholesome', u'message', u'ironically', u'mj', u'bestest', u'buddy', u'movie', u'girl', u'michael', u'jackson', u'truly', u'one', u'talented', u'people', u'ever', u'grace', u'planet', u'guilty', u'well', u'attention', u'gave', u'subject', u'hmmm', u'well', u'know', u'people', u'different', u'behind', u'closed', u'doors', u'know', u'fact', u'either', u'extremely', u'nice', u'stupid', u'guy', u'one', u'sickest', u'liars', u'hope', u'latter'], ['labeled_train_0'])\n",
      "\n",
      "Unlabeled train reviews shape: 50000\n",
      "TaggedDocument([u'watching', u'time', u'chasers', u'obvious', u'made', u'bunch', u'friends', u'maybe', u'sitting', u'around', u'one', u'day', u'film', u'school', u'said', u'hey', u'let', u'pool', u'money', u'together', u'make', u'really', u'bad', u'movie', u'something', u'like', u'ever', u'said', u'still', u'ended', u'making', u'really', u'bad', u'movie', u'dull', u'story', u'bad', u'script', u'lame', u'acting', u'poor', u'cinematography', u'bottom', u'barrel', u'stock', u'music', u'etc', u'corners', u'cut', u'except', u'one', u'would', u'prevented', u'film', u'release', u'life', u'like'], ['unlabeled_train_0'])\n",
      "\n",
      "Test reviews shape: 25000\n",
      "TaggedDocument([u'naturally', u'film', u'main', u'themes', u'mortality', u'nostalgia', u'loss', u'innocence', u'perhaps', u'surprising', u'rated', u'highly', u'older', u'viewers', u'younger', u'ones', u'however', u'craftsmanship', u'completeness', u'film', u'anyone', u'enjoy', u'pace', u'steady', u'constant', u'characters', u'full', u'engaging', u'relationships', u'interactions', u'natural', u'showing', u'need', u'floods', u'tears', u'show', u'emotion', u'screams', u'show', u'fear', u'shouting', u'show', u'dispute', u'violence', u'show', u'anger', u'naturally', u'joyce', u'short', u'story', u'lends', u'film', u'ready', u'made', u'structure', u'perfect', u'polished', u'diamond', u'small', u'changes', u'huston', u'makes', u'inclusion', u'poem', u'fit', u'neatly', u'truly', u'masterpiece', u'tact', u'subtlety', u'overwhelming', u'beauty'], ['test_0'])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess reviews\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def parse_html(data):\n",
    "    data = BeautifulSoup(data, 'lxml').get_text()\n",
    "    data = re.sub(\"[^a-zA-Z]\",\" \", data)\n",
    "    data = [x for x in data.lower().split() if not x in stop_words]\n",
    "    return data\n",
    "    \n",
    "labeled_train_reviews = []\n",
    "for i in xrange(labeled_train.shape[0]):\n",
    "    labeled_train_reviews.append(TaggedDocument(parse_html(labeled_train['review'][i]), \n",
    "                                                ['labeled_train_' + str(i)]))\n",
    "\n",
    "unlabeled_train_reviews = []\n",
    "for i in xrange(unlabeled_train.shape[0]):\n",
    "    unlabeled_train_reviews.append(TaggedDocument(parse_html(unlabeled_train['review'][i]), \n",
    "                                                  ['unlabeled_train_' + str(i)]))\n",
    "\n",
    "test_reviews = []\n",
    "for i in xrange(test.shape[0]):\n",
    "    test_reviews.append(TaggedDocument(parse_html(test['review'][i]), \n",
    "                                       ['test_' + str(i)]))\n",
    "    \n",
    "print 'Labeled train reviews shape: ' + str(len(labeled_train_reviews))\n",
    "print labeled_train_reviews[0]\n",
    "print '\\nUnlabeled train reviews shape: ' + str(len(unlabeled_train_reviews))\n",
    "print unlabeled_train_reviews[0]\n",
    "print '\\nTest reviews shape: ' + str(len(test_reviews))\n",
    "print test_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 complete\n",
      "epoch 2 complete\n",
      "epoch 3 complete\n",
      "epoch 4 complete\n",
      "epoch 5 complete\n",
      "epoch 6 complete\n",
      "epoch 7 complete\n",
      "epoch 8 complete\n",
      "epoch 9 complete\n",
      "epoch 10 complete\n"
     ]
    }
   ],
   "source": [
    "workers = 8\n",
    "\n",
    "all_reviews = labeled_train_reviews + unlabeled_train_reviews + test_reviews\n",
    "d2v = Doc2Vec(workers=workers)\n",
    "d2v.build_vocab(all_reviews)\n",
    "\n",
    "for i in range(10):\n",
    "    shuffle(all_reviews)\n",
    "    d2v.train(all_reviews)\n",
    "    print 'epoch %i complete' % (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get train and test vectors\n",
    "train_x = np.ndarray([len(labeled_train), 300])\n",
    "for i in xrange(len(labeled_train)):\n",
    "    train_x[i] = d2v.docvecs['labeled_train_' + str(i)]\n",
    "    \n",
    "train_y = np.ndarray([len(labeled_train), 2])\n",
    "for i in xrange(len(labeled_train)):    \n",
    "    train_y[i] = [1, 0] if labeled_train['sentiment'][i] == 0 else [0, 1]\n",
    "\n",
    "test_x = np.ndarray([len(test), 300])\n",
    "for i in xrange(len(test)):\n",
    "    test_x[i] = d2v.docvecs['test_' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.01524\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.01524 - acc: 0.9956 -- iter: 25000/25000\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.01524\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.01524 - acc: 0.9956 -- iter: 25000/25000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    net = tflearn.input_data(shape=[None, 300])\n",
    "    net = tflearn.fully_connected(net, 1024, 'relu')\n",
    "    net = tflearn.fully_connected(net, 128, 'relu')\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam')\n",
    "\n",
    "    # Model training\n",
    "    model = tflearn.DNN(net, tensorboard_dir='tensorboard')\n",
    "    model.fit(train_x, train_y, n_epoch=10, show_metric=True)\n",
    "    predictions = model.predict(test_x)    \n",
    "    \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":np.argmax(predictions, 1).tolist()})\n",
    "output.to_csv( \"imdb_sentiment_analysis.csv\", index=False, quoting=3 )    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
