{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: min_count, stop_words, classifier articture\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import re\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "train_pos_dir = 'aclImdb/train/pos/'\n",
    "train_neg_dir = 'aclImdb/train/neg/'\n",
    "train_unsup_dir = 'aclImdb/train/unsup/'\n",
    "test_pos_dir = 'aclImdb/test/pos/'\n",
    "test_neg_dir = 'aclImdb/test/neg/'\n",
    "\n",
    "labeled_set_size = 12500\n",
    "unlabeled_set_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 positive train reviews:\n",
      "This anime was underrated and still is. Hardly the dorky kids movie as noted, i still come back to this 10 years after i first saw it. One of the better movies released.<br /><br />The animation while not perfect is good, camera tricks give it a 3D feel and the story is still as good today even after i grew up and saw ground-breakers like Neon Genesis Evangelion and RahXephon. It has nowhere near the depth obviously but try to see it from a lighthearted view. It's a story to entertain, not to question.<br /><br />Still one of my favourites I come back too when i feel like a giggle on over more lighthearted animes. Not to say its a childish movies, there are surprisingly sad moments in this and you need a sense of humour to see it all.\n",
      "\n",
      "12500 negative train reviews:\n",
      "Whoever wrote the script for this movie does not deserve to work in Hollywood at all (not even live there), and those actors need to find another job. The most dreadful hour and some minutes of my life... and I only kept watching to see if it would get better which, unfortunately for me it did not.<br /><br />Even at the end, the credits gave me anxiety. I guess there weren't a lot of people behind the movie so they had to roll the credits slowly... very slowly.<br /><br />This movie is definitely a great \"How Not To Make a Movie\" guide. Too bad I can't give a 0.\n",
      "\n",
      "50000 unlabeled train reviews:\n",
      "The movie Contagion was a well thought out story that had below average acting and corny elements. The actual plot of the movie was great. The President of the United States is shot with the ebola virus by terrorists and the disease will spread like wildfire if time runs out. By listening to that plot, one would think that suspense keep the viewer anxious throughout the whole movie. However, the acting is so bad that it is hard to take the movie seriously. In addition, there are several effects that unsuccessfully add drama. For example, the President refuses to take the antidote provided by the terrorist cause he does not believe in it, the nurse tells Dr. Landis to tell her daughter she was \"the light of her life\" before she dies, and randomly the screen shows what hour it is since the virus was shot into the President. None of these attempts to add drama or suspense made me hold on to the edge of my seat. The bottom line was that this movie did not reach its potential. With the story line, it could have been a great science fiction; however, the bad acting and the lack of development make this a below average film even for a science fiction.\n",
      "\n",
      "12500 positive test reviews:\n",
      "I'm not sure what version of the film I saw, but it was very entertaining.<br /><br />I did not know who the \"Twins\" were (Gillian Chung and Charlene Choi) before seeing this movie and I think the English translation of the title is somewhat misleading.<br /><br />The martial arts are very nicely done. I especially liked them, because there was a lot of judo/grappling that was filmed very nicely. Donnie Yen (see him in Hero, great performance) as a director is great as he knows how to shoot these scenes.<br /><br />Everything seemed to flow for me, except there is one scene where the girls are on the rooftop fighting with bamboo poles. It has really nothing to do with the plot, but it's still entertaining.<br /><br />Overall, this is one of the better (modern) HK action flicks I've seen in a while. Although cheesy in some respects, it still pulls it off.<br /><br />Definitely a 9/10\n",
      "\n",
      "12500 negative test reviews:\n",
      "I think there's a reason this film never came close to hitting theaters. It was probably my neighbors down the street who filmed this movie with their mother's video camera. The acting is very amateur. This movie is definitely not something you would want to watch unless you were extremely bored. The actors even seem to double as directors and crew members, with no \"professionalism\" whatsoever. Should the director(s) and/or actors choose to continue with their endeavor of making movies, I would definitely advise them to brush up on their skills and perhaps take a few (ok, many) classes on film-making and acting.\n"
     ]
    }
   ],
   "source": [
    "train_pos = []\n",
    "for file_name in os.listdir(train_pos_dir):\n",
    "    with open(train_pos_dir + file_name, 'r') as myfile:\n",
    "        train_pos.append(myfile.read())\n",
    "        \n",
    "train_neg = []\n",
    "for file_name in os.listdir(train_neg_dir):\n",
    "    with open(train_neg_dir + file_name, 'r') as myfile:\n",
    "        train_neg.append(myfile.read())\n",
    "        \n",
    "train_unsup = []\n",
    "for file_name in os.listdir(train_unsup_dir):\n",
    "    with open(train_unsup_dir + file_name, 'r') as myfile:\n",
    "        train_unsup.append(myfile.read())\n",
    "        \n",
    "test_pos = []\n",
    "for file_name in os.listdir(test_pos_dir):\n",
    "    with open(test_pos_dir + file_name, 'r') as myfile:\n",
    "        test_pos.append(myfile.read())\n",
    "        \n",
    "test_neg = []\n",
    "for file_name in os.listdir(test_neg_dir):\n",
    "    with open(test_neg_dir + file_name, 'r') as myfile:\n",
    "        test_neg.append(myfile.read())     \n",
    "        \n",
    "print '%i positive train reviews:' % len(train_pos)\n",
    "print train_pos[0]\n",
    "print '\\n%i negative train reviews:' % len(train_neg)\n",
    "print train_neg[0]\n",
    "print '\\n%i unlabeled train reviews:' % len(train_unsup)\n",
    "print train_unsup[0]\n",
    "print '\\n%i positive test reviews:' % len(test_pos)\n",
    "print test_pos[0]\n",
    "print '\\n%i negative test reviews:' % len(test_neg)\n",
    "print test_neg[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 positive train reviews:\n",
      "TaggedDocument([u'anime', u'underrated', u'still', u'hardly', u'dorky', u'kids', u'movie', u'noted', u'still', u'come', u'back', u'years', u'first', u'saw', u'one', u'better', u'movies', u'released', u'animation', u'perfect', u'good', u'camera', u'tricks', u'give', u'feel', u'story', u'still', u'good', u'today', u'even', u'grew', u'saw', u'ground', u'breakers', u'like', u'neon', u'genesis', u'evangelion', u'rahxephon', u'nowhere', u'near', u'depth', u'obviously', u'try', u'see', u'lighthearted', u'view', u'story', u'entertain', u'question', u'still', u'one', u'favourites', u'come', u'back', u'feel', u'like', u'giggle', u'lighthearted', u'animes', u'say', u'childish', u'movies', u'surprisingly', u'sad', u'moments', u'need', u'sense', u'humour', u'see'], ['train_pos_0'])\n",
      "\n",
      "12500 negative train reviews:\n",
      "TaggedDocument([u'whoever', u'wrote', u'script', u'movie', u'deserve', u'work', u'hollywood', u'even', u'live', u'actors', u'need', u'find', u'another', u'job', u'dreadful', u'hour', u'minutes', u'life', u'kept', u'watching', u'see', u'would', u'get', u'better', u'unfortunately', u'even', u'end', u'credits', u'gave', u'anxiety', u'guess', u'lot', u'people', u'behind', u'movie', u'roll', u'credits', u'slowly', u'slowly', u'movie', u'definitely', u'great', u'make', u'movie', u'guide', u'bad', u'give'], ['train_neg_0'])\n",
      "\n",
      "50000 unlabeled train reviews:\n",
      "TaggedDocument([u'movie', u'contagion', u'well', u'thought', u'story', u'average', u'acting', u'corny', u'elements', u'actual', u'plot', u'movie', u'great', u'president', u'united', u'states', u'shot', u'ebola', u'virus', u'terrorists', u'disease', u'spread', u'like', u'wildfire', u'time', u'runs', u'listening', u'plot', u'one', u'would', u'think', u'suspense', u'keep', u'viewer', u'anxious', u'throughout', u'whole', u'movie', u'however', u'acting', u'bad', u'hard', u'take', u'movie', u'seriously', u'addition', u'several', u'effects', u'unsuccessfully', u'add', u'drama', u'example', u'president', u'refuses', u'take', u'antidote', u'provided', u'terrorist', u'cause', u'believe', u'nurse', u'tells', u'dr', u'landis', u'tell', u'daughter', u'light', u'life', u'dies', u'randomly', u'screen', u'shows', u'hour', u'since', u'virus', u'shot', u'president', u'none', u'attempts', u'add', u'drama', u'suspense', u'made', u'hold', u'edge', u'seat', u'bottom', u'line', u'movie', u'reach', u'potential', u'story', u'line', u'could', u'great', u'science', u'fiction', u'however', u'bad', u'acting', u'lack', u'development', u'make', u'average', u'film', u'even', u'science', u'fiction'], ['train_unsup_0'])\n",
      "\n",
      "12500 positive test reviews:\n",
      "TaggedDocument([u'sure', u'version', u'film', u'saw', u'entertaining', u'know', u'twins', u'gillian', u'chung', u'charlene', u'choi', u'seeing', u'movie', u'think', u'english', u'translation', u'title', u'somewhat', u'misleading', u'martial', u'arts', u'nicely', u'done', u'especially', u'liked', u'lot', u'judo', u'grappling', u'filmed', u'nicely', u'donnie', u'yen', u'see', u'hero', u'great', u'performance', u'director', u'great', u'knows', u'shoot', u'scenes', u'everything', u'seemed', u'flow', u'except', u'one', u'scene', u'girls', u'rooftop', u'fighting', u'bamboo', u'poles', u'really', u'nothing', u'plot', u'still', u'entertaining', u'overall', u'one', u'better', u'modern', u'hk', u'action', u'flicks', u'seen', u'although', u'cheesy', u'respects', u'still', u'pulls', u'definitely'], ['test_pos_0'])\n",
      "\n",
      "12500 negative test reviews:\n",
      "TaggedDocument([u'think', u'reason', u'film', u'never', u'came', u'close', u'hitting', u'theaters', u'probably', u'neighbors', u'street', u'filmed', u'movie', u'mother', u'video', u'camera', u'acting', u'amateur', u'movie', u'definitely', u'something', u'would', u'want', u'watch', u'unless', u'extremely', u'bored', u'actors', u'even', u'seem', u'double', u'directors', u'crew', u'members', u'professionalism', u'whatsoever', u'director', u'actors', u'choose', u'continue', u'endeavor', u'making', u'movies', u'would', u'definitely', u'advise', u'brush', u'skills', u'perhaps', u'take', u'ok', u'many', u'classes', u'film', u'making', u'acting'], ['test_neg_0'])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess reviews\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def parse_html(data):\n",
    "    data = BeautifulSoup(data, 'lxml').get_text()\n",
    "    data = re.sub(\"[^a-zA-Z]\",\" \", data)\n",
    "    data = [x for x in data.lower().split() if not x in stop_words]\n",
    "    return data\n",
    "    \n",
    "for i in xrange(labeled_set_size):\n",
    "    train_pos[i] = TaggedDocument(parse_html(train_pos[i]), ['train_pos_' + str(i)])\n",
    "    train_neg[i] = TaggedDocument(parse_html(train_neg[i]), ['train_neg_' + str(i)])\n",
    "    test_pos[i] = TaggedDocument(parse_html(test_pos[i]), ['test_pos_' + str(i)])\n",
    "    test_neg[i] = TaggedDocument(parse_html(test_neg[i]), ['test_neg_' + str(i)])\n",
    "    \n",
    "for i in xrange(unlabeled_set_size):\n",
    "    train_unsup[i] = TaggedDocument(parse_html(train_unsup[i]), ['train_unsup_' + str(i)])\n",
    "    \n",
    "print '%i positive train reviews:' % len(train_pos)\n",
    "print train_pos[0]\n",
    "print '\\n%i negative train reviews:' % len(train_neg)\n",
    "print train_neg[0]\n",
    "print '\\n%i unlabeled train reviews:' % len(train_unsup)\n",
    "print train_unsup[0]\n",
    "print '\\n%i positive test reviews:' % len(test_pos)\n",
    "print test_pos[0]\n",
    "print '\\n%i negative test reviews:' % len(test_neg)\n",
    "print test_neg[0]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 complete\n",
      "epoch 2 complete\n",
      "epoch 3 complete\n",
      "epoch 4 complete\n",
      "epoch 5 complete\n",
      "epoch 6 complete\n",
      "epoch 7 complete\n",
      "epoch 8 complete\n",
      "epoch 9 complete\n",
      "epoch 10 complete\n"
     ]
    }
   ],
   "source": [
    "workers = 8\n",
    "min_count = 30 # 30 is the max number of reviews per movie in the dataset\n",
    "\n",
    "all_reviews = train_pos + train_neg + train_unsup + test_pos + test_neg\n",
    "d2v = Doc2Vec(workers=workers, min_count=min_count)\n",
    "d2v.build_vocab(all_reviews)\n",
    "\n",
    "for i in range(10):\n",
    "    shuffle(all_reviews)\n",
    "    d2v.train(all_reviews)\n",
    "    print 'epoch %i complete' % (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get train and test vectors\n",
    "x_vector_size = 300\n",
    "y_vector_size = 2\n",
    "\n",
    "train_x = np.ndarray([2 * labeled_set_size, x_vector_size])\n",
    "train_y = np.ndarray([2 * labeled_set_size, y_vector_size])\n",
    "test_x = np.ndarray([2 * labeled_set_size, x_vector_size])\n",
    "test_y = np.ndarray([2 * labeled_set_size, y_vector_size])\n",
    "\n",
    "for i in xrange(labeled_set_size):\n",
    "    train_x[i] = d2v.docvecs['train_pos_' + str(i)]\n",
    "    train_y[i] = [1, 0]\n",
    "    test_x[i] = d2v.docvecs['test_pos_' + str(i)]    \n",
    "    test_y[i] = [1, 0]\n",
    "    \n",
    "for i in xrange(labeled_set_size):\n",
    "    train_x[i + labeled_set_size] = d2v.docvecs['train_neg_' + str(i)]\n",
    "    train_y[i + labeled_set_size] = [0, 1]\n",
    "    test_x[i + labeled_set_size] = d2v.docvecs['test_neg_' + str(i)]    \n",
    "    test_y[i + labeled_set_size] = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.68755\u001b[0m\u001b[0m\n",
      "\u001b[2K\r",
      "| Adam | epoch: 000 | loss: 0.68755 -- iter: 00640/25000\n"
     ]
    }
   ],
   "source": [
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    net = tflearn.input_data(shape=[None, 300])\n",
    "    net = tflearn.fully_connected(net, 1024, 'relu')\n",
    "    net = tflearn.fully_connected(net, 128, 'relu')\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam')\n",
    "\n",
    "    # Model training\n",
    "    model = tflearn.DNN(net, tensorboard_dir='tensorboard')\n",
    "    model.fit(train_x, train_y, n_epoch=10)\n",
    "    predictions = model.predict(test_x)\n",
    "    \n",
    "    print 'Train set accuracy ' + '{:.2%}'.format(model.evaluate(train_x, train_y)[0])\n",
    "    print 'Test set accuracy ' + '{:.2%}'.format(model.evaluate(test_x, test_y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditions for positive test reviews (shold be all 0s):\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      "\n",
      "Preditions for negative test reviews (shold be all 1s):\n",
      "[1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
      " 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
      " 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
      " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1\n",
      " 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print 'Preditions for positive test reviews (shold be all 0s):'\n",
    "print np.argmax(predictions, 1)[:999]\n",
    "print '\\nPreditions for negative test reviews (shold be all 1s):'\n",
    "print np.argmax(predictions, 1)[24001:]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
