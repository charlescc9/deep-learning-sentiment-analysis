{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "Sentiment Aanlysis with gensim and TFlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import re\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "# Define directories\n",
    "train_pos_dir = 'aclImdb/train/pos/'\n",
    "train_neg_dir = 'aclImdb/train/neg/'\n",
    "train_unsup_dir = 'aclImdb/train/unsup/'\n",
    "test_pos_dir = 'aclImdb/test/pos/'\n",
    "test_neg_dir = 'aclImdb/test/neg/'\n",
    "\n",
    "# Define dataset sizes\n",
    "labeled_set_size = 12500\n",
    "unlabeled_set_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 positive train reviews:\n",
      "This anime was underrated and still is. Hardly the dorky kids movie as noted, i still come back to t\n",
      "\n",
      "12500 negative train reviews:\n",
      "Whoever wrote the script for this movie does not deserve to work in Hollywood at all (not even live \n",
      "\n",
      "50000 unlabeled train reviews:\n",
      "The movie Contagion was a well thought out story that had below average acting and corny elements. T\n",
      "\n",
      "12500 positive test reviews:\n",
      "I'm not sure what version of the film I saw, but it was very entertaining.<br /><br />I did not know\n",
      "\n",
      "12500 negative test reviews:\n",
      "I think there's a reason this film never came close to hitting theaters. It was probably my neighbor\n"
     ]
    }
   ],
   "source": [
    "# Read and explore data\n",
    "train_pos = []\n",
    "for file_name in os.listdir(train_pos_dir):\n",
    "    with open(train_pos_dir + file_name, 'r') as myfile:\n",
    "        train_pos.append(myfile.read())\n",
    "        \n",
    "train_neg = []\n",
    "for file_name in os.listdir(train_neg_dir):\n",
    "    with open(train_neg_dir + file_name, 'r') as myfile:\n",
    "        train_neg.append(myfile.read())\n",
    "        \n",
    "train_unsup = []\n",
    "for file_name in os.listdir(train_unsup_dir):\n",
    "    with open(train_unsup_dir + file_name, 'r') as myfile:\n",
    "        train_unsup.append(myfile.read())\n",
    "        \n",
    "test_pos = []\n",
    "for file_name in os.listdir(test_pos_dir):\n",
    "    with open(test_pos_dir + file_name, 'r') as myfile:\n",
    "        test_pos.append(myfile.read())\n",
    "        \n",
    "test_neg = []\n",
    "for file_name in os.listdir(test_neg_dir):\n",
    "    with open(test_neg_dir + file_name, 'r') as myfile:\n",
    "        test_neg.append(myfile.read())     \n",
    "        \n",
    "print '%i positive train reviews:' % len(train_pos)\n",
    "print train_pos[0][:100]\n",
    "print '\\n%i negative train reviews:' % len(train_neg)\n",
    "print train_neg[0][:100]\n",
    "print '\\n%i unlabeled train reviews:' % len(train_unsup)\n",
    "print train_unsup[0][:100]\n",
    "print '\\n%i positive test reviews:' % len(test_pos)\n",
    "print test_pos[0][:100]\n",
    "print '\\n%i negative test reviews:' % len(test_neg)\n",
    "print test_neg[0][:100]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 positive train reviews:\n",
      "[u'anime', u'underrated', u'still', u'hardly', u'dorky', u'kids', u'movie', u'noted']\n",
      "\n",
      "12500 negative train reviews:\n",
      "[u'whoever', u'wrote', u'script', u'movie', u'deserve', u'work', u'hollywood', u'even']\n",
      "\n",
      "50000 unlabeled train reviews:\n",
      "[u'movie', u'contagion', u'well', u'thought', u'story', u'average', u'acting', u'corny']\n",
      "\n",
      "12500 positive test reviews:\n",
      "[u'sure', u'version', u'film', u'saw', u'entertaining', u'know', u'twins', u'gillian']\n",
      "\n",
      "12500 negative test reviews:\n",
      "[u'think', u'reason', u'film', u'never', u'came', u'close', u'hitting', u'theaters']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data so reviews can be embedded\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def parse_html(data):\n",
    "    data = BeautifulSoup(data, 'lxml').get_text() # Remove markup\n",
    "    data = re.sub(\"[^a-zA-Z]\",\" \", data) # Remove all non-alphanumeric characters\n",
    "    data = [x for x in data.lower().split() if not x in stop_words] # Remove stopwords\n",
    "    return data\n",
    "    \n",
    "for i in xrange(labeled_set_size):\n",
    "    train_pos[i] = TaggedDocument(parse_html(train_pos[i]), ['train_pos_' + str(i)])\n",
    "    train_neg[i] = TaggedDocument(parse_html(train_neg[i]), ['train_neg_' + str(i)])\n",
    "    test_pos[i] = TaggedDocument(parse_html(test_pos[i]), ['test_pos_' + str(i)])\n",
    "    test_neg[i] = TaggedDocument(parse_html(test_neg[i]), ['test_neg_' + str(i)])\n",
    "    \n",
    "for i in xrange(unlabeled_set_size):\n",
    "    train_unsup[i] = TaggedDocument(parse_html(train_unsup[i]), ['train_unsup_' + str(i)])\n",
    "    \n",
    "print '%i positive train reviews:' % len(train_pos)\n",
    "print train_pos[0][0][:8]\n",
    "print '\\n%i negative train reviews:' % len(train_neg)\n",
    "print train_neg[0][0][:8]\n",
    "print '\\n%i unlabeled train reviews:' % len(train_unsup)\n",
    "print train_unsup[0][0][:8]\n",
    "print '\\n%i positive test reviews:' % len(test_pos)\n",
    "print test_pos[0][0][:8]\n",
    "print '\\n%i negative test reviews:' % len(test_neg)\n",
    "print test_neg[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embed documents using doc2vec\n",
    "if os.path.isfile('d2v'):\n",
    "    d2v = Doc2Vec.load('d2v')\n",
    "else:\n",
    "    workers = 8 # Number of virtual CPU cores on machine\n",
    "    window = 16 # Skip-gram window\n",
    "    min_count = 30 # 30 is the max number of reviews per movie in the dataset\n",
    "\n",
    "    all_reviews = train_pos + train_neg + train_unsup + test_pos + test_neg\n",
    "    d2v = Doc2Vec(window=window, workers=workers, min_count=min_count)\n",
    "    d2v.build_vocab(all_reviews)\n",
    "\n",
    "    for i in range(10):\n",
    "        shuffle(all_reviews) # Make sure to shuffle each epoch\n",
    "        d2v.train(all_reviews)\n",
    "        print 'epoch %i complete' % (i + 1)\n",
    "        \n",
    "    d2v.save('d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to man: \n",
      "[(u'lady', 0.7224129438400269), (u'woman', 0.6937535405158997), (u'guy', 0.6755703687667847)]\n",
      "\n",
      "Most similar to movie: \n",
      "[(u'film', 0.9590884447097778), (u'flick', 0.7753545641899109), (u'show', 0.7212437987327576)]\n"
     ]
    }
   ],
   "source": [
    "# Examine embedding\n",
    "print 'Most similar to man: '\n",
    "print d2v.most_similar('man')[:3]\n",
    "print '\\nMost similar to movie: '\n",
    "print d2v.most_similar('movie')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get train and test embedded vectors for classification\n",
    "x_vector_size = 300\n",
    "y_vector_size = 2\n",
    "\n",
    "train_x = np.ndarray([2 * labeled_set_size, x_vector_size])\n",
    "train_y = np.ndarray([2 * labeled_set_size, y_vector_size])\n",
    "test_x = np.ndarray([2 * labeled_set_size, x_vector_size])\n",
    "test_y = np.ndarray([2 * labeled_set_size, y_vector_size])\n",
    "\n",
    "for i in xrange(labeled_set_size):\n",
    "    train_x[i] = d2v.docvecs['train_pos_' + str(i)]\n",
    "    train_y[i] = [1, 0]\n",
    "    test_x[i] = d2v.docvecs['test_pos_' + str(i)]    \n",
    "    test_y[i] = [1, 0]\n",
    "    \n",
    "for i in xrange(labeled_set_size):\n",
    "    train_x[i + labeled_set_size] = d2v.docvecs['train_neg_' + str(i)]\n",
    "    train_y[i + labeled_set_size] = [0, 1]\n",
    "    test_x[i + labeled_set_size] = d2v.docvecs['test_neg_' + str(i)]    \n",
    "    test_y[i + labeled_set_size] = [0, 1]\n",
    "    \n",
    "# Shuffle data\n",
    "shuffled_i = np.random.permutation(np.arange(len(train_x)))\n",
    "train_x = train_x[shuffled_i]\n",
    "train_y = train_y[shuffled_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.24688\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.24688 - acc: 0.9095 | val_loss: 0.30798 - val_acc: 0.8707 -- iter: 25000/25000\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.24688\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.24688 - acc: 0.9095 | val_loss: 0.30798 - val_acc: 0.8707 -- iter: 25000/25000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# DNN model\n",
    "epochs = 10\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    net = tflearn.input_data(shape=[None, x_vector_size])\n",
    "    net = tflearn.fully_connected(net, 128, 'relu')\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam')\n",
    "\n",
    "    model = tflearn.DNN(net, tensorboard_dir='tensorboard')\n",
    "    model.fit(train_x, train_y, epochs, (test_x, test_y), True)\n",
    "    dnn_score = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.14893\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.14893 - acc: 0.9605 | val_loss: 0.43279 - val_acc: 0.8504 -- iter: 25000/25000\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.14893\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.14893 - acc: 0.9605 | val_loss: 0.43279 - val_acc: 0.8504 -- iter: 25000/25000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "epochs = 10\n",
    "timesteps = 10\n",
    "train_x_lstm = np.stack([train_x] * timesteps, 1)\n",
    "test_x_lstm = np.stack([test_x] * timesteps, 1)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    net = tflearn.input_data([None, timesteps, x_vector_size])\n",
    "    net = tflearn.lstm(net, x_vector_size)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam')\n",
    "\n",
    "    model = tflearn.DNN(net, tensorboard_dir='tensorboard')\n",
    "    model.fit(train_x_lstm, train_y, epochs, (test_x_lstm, test_y), True)\n",
    "    lstm_score = model.evaluate(test_x_lstm, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN model test set accuracy 87.07%\n",
      "LSTM model test set accuracy 85.04%\n"
     ]
    }
   ],
   "source": [
    "# Performance\n",
    "print 'DNN model test set accuracy ' + '{:.2%}'.format(dnn_score[0])\n",
    "print 'LSTM model test set accuracy ' + '{:.2%}'.format(lstm_score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Implement more regularization to combat overfitting\n",
    "* Compare tf-idf performance to doc2vec\n",
    "* Compare performance with stopwords\n",
    "* Create TSNE visualization for document embeddings\n",
    "* Develop CNN classification model\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
